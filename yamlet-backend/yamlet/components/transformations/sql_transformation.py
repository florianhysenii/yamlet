import logging
from pyspark.sql import DataFrame
from .base_transformation import BaseTransformation

logger = logging.getLogger(__name__)

class SQLTransformation(BaseTransformation):
    """
    Executes a user-specified SQL block on a Spark DataFrame.
    
    This transformation registers the input DataFrame as a temporary view (default: 'temp_table'),
    then sequentially executes each SQL statement provided in the SQL block. The final SQL statement's
    result is returned as the transformed DataFrame.
    
    The SQL block can include multiple statements separated by semicolons.
    For example:
    
    sql_query: |
      CREATE OR REPLACE TEMP VIEW enriched AS
      SELECT id, UPPER(name) AS name, score,
             CASE WHEN score > 100 THEN 'High' ELSE 'Low' END AS score_category
      FROM temp_table;
      SELECT * FROM enriched;
    
    This block first creates a view named 'enriched' and then returns its contents.
    """
    def __init__(self, sql_query: str, temp_view_name: str = "temp_table"):
        """
        Initializes the SQLTransformation.
        
        Args:
            sql_query (str): A block of SQL statements separated by semicolons.
            temp_view_name (str, optional): Name of the temporary view for the input DataFrame.
                                             Defaults to "temp_table".
        """
        self.sql_query = sql_query
        self.temp_view_name = temp_view_name

    def apply(self, df: DataFrame) -> DataFrame:
        """
        Registers the input DataFrame as a temporary view, then executes the SQL block.
        
        Returns:
            DataFrame: The result of the final SQL statement.
        """
        logger.info(f"Registering DataFrame as temporary view '{self.temp_view_name}'.")
        df.createOrReplaceTempView(self.temp_view_name)
        
        statements = [stmt.strip() for stmt in self.sql_query.strip().split(";") if stmt.strip()]
        if not statements:
            logger.error("No valid SQL statements found in the SQL block.")
            raise ValueError("SQL block is empty after parsing.")
        
        result_df = None
        for i, stmt in enumerate(statements):
            logger.info(f"Executing SQL statement {i + 1}/{len(statements)}: {stmt[:80]}...")
            try:
                result_df = df.sparkSession.sql(stmt)
            except Exception as e:
                logger.error(f"Error executing SQL statement {i + 1}: {e}")
                raise e
        if result_df is None:
            logger.error("SQLTransformation did not produce any output DataFrame.")
            raise ValueError("No output DataFrame generated by SQLTransformation.")
        return result_df
